{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        donald trump sends out embarrassing new year’s...\n",
      "1        drunk bragging trump staffer started russian c...\n",
      "2        sheriff david clarke becomes an internet joke ...\n",
      "3        trump is so obsessed he even has obama’s name ...\n",
      "4        pope francis just called out donald trump duri...\n",
      "                               ...                        \n",
      "38653    'fully committed' nato backs new us approach a...\n",
      "38654    lexisnexis withdrew two products chinese marke...\n",
      "38655    minsk cultural hub becomes authorities in shad...\n",
      "38656    vatican upbeat possibility pope francis visiti...\n",
      "38657    indonesia buy $114 billion worth russian jets ...\n",
      "Name: overall_content, Length: 38658, dtype: object 0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "38653    0\n",
      "38654    0\n",
      "38655    0\n",
      "38656    0\n",
      "38657    0\n",
      "Name: class, Length: 38658, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"master_dataset/processed_data.csv\")\n",
    "# drop unwanted features\n",
    "\n",
    "\n",
    "# Drop all the column , keep only class , overall_context\n",
    "\n",
    "x = data['overall_content']\n",
    "y = data['class'] \n",
    "    \n",
    "# As we will be vectorizing the content and doing LSTM on it\n",
    "\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21196\n",
       "1    17462\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12717\n",
       "1    10477\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first split the dataset into training and test sets\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 4222)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size = 0.25, random_state = 4222)\n",
    "\n",
    "#check that train set is balance\n",
    "y_train.value_counts()\n",
    "\n",
    "# Since the dataset is pretty balanced, Real - 55% and Fake - 45% of the data,\n",
    "# By oversampling, we will have duplicates in the model which will overtrain out model.\n",
    "# By undersampling, we might lose out on critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of maximum text \n",
    "lst = []\n",
    "words = []\n",
    "for item in x_train:\n",
    "    lst.append(len(item.split()))\n",
    "\n",
    "big_list = [item.split() for item in x_train]\n",
    "flat_list = [item for sublist in big_list for item in sublist]\n",
    "unique = list(set(flat_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207463\n",
      "5130\n"
     ]
    }
   ],
   "source": [
    "print(len(unique)) # How many different words are there\n",
    "print(max(lst)) # What the maximum word in an article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.1596e+04, 1.4800e+03, 7.0000e+01, 2.4000e+01, 1.2000e+01,\n",
       "        4.0000e+00, 3.0000e+00, 3.0000e+00, 0.0000e+00, 2.0000e+00]),\n",
       " array([2.0000e+00, 5.1480e+02, 1.0276e+03, 1.5404e+03, 2.0532e+03,\n",
       "        2.5660e+03, 3.0788e+03, 3.5916e+03, 4.1044e+03, 4.6172e+03,\n",
       "        5.1300e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model constants / Parameters\n",
    "max_features = 40000\n",
    "embed_size = 100\n",
    "maxlen = [250,500,1000]\n",
    "dense_units = [64,128]\n",
    "lstm_units = [64,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_params = list(itertools.product(maxlen,dense_units,lstm_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(250, 64, 64),\n",
       " (250, 64, 128),\n",
       " (250, 128, 64),\n",
       " (250, 128, 128),\n",
       " (500, 64, 64),\n",
       " (500, 64, 128),\n",
       " (500, 128, 64),\n",
       " (500, 128, 128),\n",
       " (1000, 64, 64),\n",
       " (1000, 64, 128),\n",
       " (1000, 128, 64),\n",
       " (1000, 128, 128)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_sets(maxlen):\n",
    "    #result = []\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(x_train.squeeze())\n",
    "\n",
    "    tokenized_train = tokenizer.texts_to_sequences(x_train.squeeze())\n",
    "    x_train_tokenized = pad_sequences(tokenized_train , maxlen=maxlen)\n",
    "    \n",
    "    tokenized_val = tokenizer.texts_to_sequences(x_validation)\n",
    "    x_validation_tokenized = pad_sequences(tokenized_val , maxlen=maxlen)\n",
    "    \n",
    "    tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "    x_test_tokenized = pad_sequences(tokenized_test , maxlen=maxlen)\n",
    "\n",
    "    return(x_train_tokenized,x_validation_tokenized,x_test_tokenized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length: 250 , Dense Unit 64 , LSTM Unit 64   \n",
      "725/725 [==============================] - 117s 158ms/step - loss: 0.1336 - accuracy: 0.9431 - val_loss: 0.0339 - val_accuracy: 0.9893\n",
      "Max Length: 250 , Dense Unit 64 , LSTM Unit 128   \n",
      "725/725 [==============================] - 188s 256ms/step - loss: 0.1632 - accuracy: 0.9331 - val_loss: 0.0395 - val_accuracy: 0.9865\n",
      "Max Length: 250 , Dense Unit 128 , LSTM Unit 64   \n",
      "725/725 [==============================] - 121s 164ms/step - loss: 0.1634 - accuracy: 0.9267 - val_loss: 0.0380 - val_accuracy: 0.9878\n",
      "Max Length: 250 , Dense Unit 128 , LSTM Unit 128   \n",
      "725/725 [==============================] - 179s 245ms/step - loss: 0.1613 - accuracy: 0.9344 - val_loss: 0.0416 - val_accuracy: 0.9869\n",
      "Max Length: 500 , Dense Unit 64 , LSTM Unit 64   \n",
      "725/725 [==============================] - 198s 271ms/step - loss: 0.1379 - accuracy: 0.9409 - val_loss: 0.0232 - val_accuracy: 0.9924\n",
      "Max Length: 500 , Dense Unit 64 , LSTM Unit 128   \n",
      "725/725 [==============================] - 347s 477ms/step - loss: 0.1104 - accuracy: 0.9527 - val_loss: 0.0160 - val_accuracy: 0.9950\n",
      "Max Length: 500 , Dense Unit 128 , LSTM Unit 64   \n",
      "725/725 [==============================] - 204s 279ms/step - loss: 0.1248 - accuracy: 0.9466 - val_loss: 0.0176 - val_accuracy: 0.9951\n",
      "Max Length: 500 , Dense Unit 128 , LSTM Unit 128   \n",
      "725/725 [==============================] - 348s 477ms/step - loss: 0.1395 - accuracy: 0.9418 - val_loss: 0.0398 - val_accuracy: 0.9841\n",
      "Max Length: 1000 , Dense Unit 64 , LSTM Unit 64   \n",
      "725/725 [==============================] - 376s 516ms/step - loss: 0.1599 - accuracy: 0.9370 - val_loss: 0.0185 - val_accuracy: 0.9944\n",
      "Max Length: 1000 , Dense Unit 64 , LSTM Unit 128   \n",
      "725/725 [==============================] - 803s 1s/step - loss: 0.1634 - accuracy: 0.9233 - val_loss: 0.0250 - val_accuracy: 0.9922\n",
      "Max Length: 1000 , Dense Unit 128 , LSTM Unit 64   \n",
      "725/725 [==============================] - 379s 520ms/step - loss: 0.1128 - accuracy: 0.9495 - val_loss: 0.0270 - val_accuracy: 0.9924\n",
      "Max Length: 1000 , Dense Unit 128 , LSTM Unit 128   \n",
      "725/725 [==============================] - 731s 1s/step - loss: 0.1138 - accuracy: 0.9516 - val_loss: 0.0133 - val_accuracy: 0.9959\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(candidate_params)):\n",
    "    maxlen = candidate_params[i][0]\n",
    "    dense_units = candidate_params[i][1]\n",
    "    lstm_units = candidate_params[i][2]\n",
    "\n",
    "\n",
    "    result = tokenized_sets(maxlen)\n",
    "\n",
    "    x_train_tokenized = result[0]\n",
    "    x_validation_tokenized = result[1]\n",
    "    x_test_tokenized = result[2]\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Embedding(max_features,embed_size))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.LSTM(lstm_units, return_sequences=True))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1,activation=\"sigmoid\",name=\"predictions\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "    print(\"Max Length: {} , Dense Unit {} , LSTM Unit {}   \".format( candidate_params[i][0], candidate_params[i][1] , candidate_params[i][2]))\n",
    "\n",
    "    model.fit(x_train_tokenized, y_train,validation_data=(x_validation_tokenized,y_validation))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "725/725 [==============================] - 280s 383ms/step - loss: 0.1301 - accuracy: 0.9454 - val_loss: 0.0246 - val_accuracy: 0.9916\n",
      "Epoch 2/2\n",
      "725/725 [==============================] - 345s 475ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.0196 - val_accuracy: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2402c6b0a60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenized_sets(500)\n",
    "\n",
    "x_train_tokenized = result[0]\n",
    "x_validation_tokenized = result[1]\n",
    "x_test_tokenized = result[2]\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features,embed_size))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1,activation=\"sigmoid\",name=\"predictions\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_tokenized, y_train,validation_data=(x_validation_tokenized,y_validation), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 31s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction if the news is fake \n",
    "# Class 1 (Fake) if predicted prob >= 0.5, else class 0 (Real)\n",
    "\n",
    "y_pred = (model.predict(x_test_tokenized) >= 0.5).astype(\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      4235\n",
      "           1       1.00      0.99      0.99      3497\n",
      "\n",
      "    accuracy                           0.99      7732\n",
      "   macro avg       1.00      0.99      0.99      7732\n",
      "weighted avg       0.99      0.99      0.99      7732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis after Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725/725 [==============================] - 93s 129ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Accuracy of the model on Training Data is -  99.97413158416748 %\n",
      "242/242 [==============================] - 31s 127ms/step - loss: 0.0196 - accuracy: 0.9942\n",
      "Accuracy of the model on Validation Data is -  99.41800236701965 %\n",
      "242/242 [==============================] - 31s 128ms/step - loss: 0.0173 - accuracy: 0.9950\n",
      "Accuracy of the model on Testing Data is -  99.49560165405273 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train_tokenized,y_train)[1]*100 ,  \"%\")\n",
    "print(\"Accuracy of the model on Validation Data is - \" , model.evaluate(x_validation_tokenized,y_validation)[1]*100 , \"%\")\n",
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test_tokenized,y_test)[1]*100 , \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
