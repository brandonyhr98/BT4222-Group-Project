{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>class</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>title_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Donald Trump wish Americans Happy New Year lea...</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>On Friday, revealed former Milwaukee Sheriff D...</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>On Christmas day, Donald Trump announced would...</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Pope Francis used annual Christmas Day message...</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Duri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  class  \\\n",
       "0  December 31, 2017      1   \n",
       "1  December 31, 2017      1   \n",
       "2  December 30, 2017      1   \n",
       "3  December 29, 2017      1   \n",
       "4  December 25, 2017      1   \n",
       "\n",
       "                              text_without_stopwords  \\\n",
       "0  Donald Trump wish Americans Happy New Year lea...   \n",
       "1  House Intelligence Committee Chairman Devin Nu...   \n",
       "2  On Friday, revealed former Milwaukee Sheriff D...   \n",
       "3  On Christmas day, Donald Trump announced would...   \n",
       "4  Pope Francis used annual Christmas Day message...   \n",
       "\n",
       "                             title_without_stopwords  \n",
       "0  Donald Trump Sends Out Embarrassing New Year’s...  \n",
       "1  Drunk Bragging Trump Staffer Started Russian C...  \n",
       "2  Sheriff David Clarke Becomes An Internet Joke ...  \n",
       "3  Trump Is So Obsessed He Even Has Obama’s Name ...  \n",
       "4  Pope Francis Just Called Out Donald Trump Duri...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re , nltk , string , gensim , spacy, pyLDAvis , syllables , pyLDAvis.gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel , TfidfModel \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "data = pd.read_csv(\"master_dataset/merged_cleaned.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text-based features\n",
    "#word count\n",
    "\n",
    "\n",
    "def word_count(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.replace(\"\\r\\n\", ' ')\n",
    "    if text == \"no title\":\n",
    "        return 0\n",
    "    else:\n",
    "        return len(str(text).split(' '))\n",
    "\n",
    "def sentence_count(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def average_word_length(text):\n",
    "    words = text.split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        count += len(word)\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return count / len(words)\n",
    "        \n",
    "def punctuation_count(text):\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word in string.punctuation:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def stopword_count(text):\n",
    "    stopword = stopwords.words('english')\n",
    "    count = 0\n",
    "    for word in text.split():\n",
    "        if word in stopword:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "#change data type to string\n",
    "data['text'] = data['text'].astype(str)\n",
    "data['title'] = data['title'].astype(str)\n",
    "\n",
    "data['text_word_count'] = data['text'].apply(word_count)\n",
    "data['title_word_count'] = data['title'].apply(word_count)\n",
    "\n",
    "data['text_sentence_count'] = data['text'].apply(sentence_count)\n",
    "data['title_sentence_count'] = data['title'].apply(sentence_count)\n",
    "\n",
    "data['text_average_word_length'] = data['text'].apply(average_word_length)\n",
    "data['title_average_word_length'] = data['title'].apply(average_word_length)\n",
    "\n",
    "data['text_punctuation_count'] = data['text'].apply(punctuation_count)\n",
    "data['title_punctuation_count'] = data['title'].apply(punctuation_count)\n",
    "\n",
    "data['text_stopwords_count'] = data['text'].apply(stopword_count)\n",
    "data['title_stopwords_count'] = data['title'].apply(stopword_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>class</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>title_without_stopwords</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>title_sentence_count</th>\n",
       "      <th>text_average_word_length</th>\n",
       "      <th>title_average_word_length</th>\n",
       "      <th>text_punctuation_count</th>\n",
       "      <th>title_punctuation_count</th>\n",
       "      <th>text_stopwords_count</th>\n",
       "      <th>title_stopwords_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Donald Trump wish Americans Happy New Year lea...</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’s...</td>\n",
       "      <td>516</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>4.804040</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian C...</td>\n",
       "      <td>309</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5.213115</td>\n",
       "      <td>7.625000</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>On Friday, revealed former Milwaukee Sheriff D...</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke ...</td>\n",
       "      <td>600</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5.168966</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>On Christmas day, Donald Trump announced would...</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name ...</td>\n",
       "      <td>475</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5.180180</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Pope Francis used annual Christmas Day message...</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Duri...</td>\n",
       "      <td>434</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>4.554762</td>\n",
       "      <td>5.363636</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39100</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>NATO allies on Tuesday welcomed President Dona...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>NATO allies Tuesday welcomed President Donald ...</td>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>482</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5.008639</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39101</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LexisNexis, a provider of legal, regulatory an...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>LexisNexis, provider legal, regulatory busines...</td>\n",
       "      <td>LexisNexis withdrew two products Chinese market</td>\n",
       "      <td>131</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5.336066</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39102</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>In the shadow of disused Soviet-era factories ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>In shadow disused Soviet-era factories Minsk, ...</td>\n",
       "      <td>Minsk cultural hub becomes authorities</td>\n",
       "      <td>334</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>5.044164</td>\n",
       "      <td>6.142857</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39103</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>Vatican Secretary of State Cardinal Pietro Par...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>Vatican Secretary State Cardinal Pietro Paroli...</td>\n",
       "      <td>Vatican upbeat possibility Pope Francis visiti...</td>\n",
       "      <td>210</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4.806931</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39104</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>Indonesia will buy 11 Sukhoi fighter jets wort...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>Indonesia buy 11 Sukhoi fighter jets worth $1....</td>\n",
       "      <td>Indonesia buy $1.14 billion worth Russian jets</td>\n",
       "      <td>209</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5.362319</td>\n",
       "      <td>4.888889</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39105 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "39100  'Fully committed' NATO backs new U.S. approach...   \n",
       "39101  LexisNexis withdrew two products from Chinese ...   \n",
       "39102  Minsk cultural hub becomes haven from authorities   \n",
       "39103  Vatican upbeat on possibility of Pope Francis ...   \n",
       "39104  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "39100  NATO allies on Tuesday welcomed President Dona...  worldnews   \n",
       "39101  LexisNexis, a provider of legal, regulatory an...  worldnews   \n",
       "39102  In the shadow of disused Soviet-era factories ...  worldnews   \n",
       "39103  Vatican Secretary of State Cardinal Pietro Par...  worldnews   \n",
       "39104  Indonesia will buy 11 Sukhoi fighter jets wort...  worldnews   \n",
       "\n",
       "                    date  class  \\\n",
       "0      December 31, 2017      1   \n",
       "1      December 31, 2017      1   \n",
       "2      December 30, 2017      1   \n",
       "3      December 29, 2017      1   \n",
       "4      December 25, 2017      1   \n",
       "...                  ...    ...   \n",
       "39100   August 22, 2017       0   \n",
       "39101   August 22, 2017       0   \n",
       "39102   August 22, 2017       0   \n",
       "39103   August 22, 2017       0   \n",
       "39104   August 22, 2017       0   \n",
       "\n",
       "                                  text_without_stopwords  \\\n",
       "0      Donald Trump wish Americans Happy New Year lea...   \n",
       "1      House Intelligence Committee Chairman Devin Nu...   \n",
       "2      On Friday, revealed former Milwaukee Sheriff D...   \n",
       "3      On Christmas day, Donald Trump announced would...   \n",
       "4      Pope Francis used annual Christmas Day message...   \n",
       "...                                                  ...   \n",
       "39100  NATO allies Tuesday welcomed President Donald ...   \n",
       "39101  LexisNexis, provider legal, regulatory busines...   \n",
       "39102  In shadow disused Soviet-era factories Minsk, ...   \n",
       "39103  Vatican Secretary State Cardinal Pietro Paroli...   \n",
       "39104  Indonesia buy 11 Sukhoi fighter jets worth $1....   \n",
       "\n",
       "                                 title_without_stopwords  text_word_count  \\\n",
       "0      Donald Trump Sends Out Embarrassing New Year’s...              516   \n",
       "1      Drunk Bragging Trump Staffer Started Russian C...              309   \n",
       "2      Sheriff David Clarke Becomes An Internet Joke ...              600   \n",
       "3      Trump Is So Obsessed He Even Has Obama’s Name ...              475   \n",
       "4      Pope Francis Just Called Out Donald Trump Duri...              434   \n",
       "...                                                  ...              ...   \n",
       "39100  'Fully committed' NATO backs new U.S. approach...              482   \n",
       "39101    LexisNexis withdrew two products Chinese market              131   \n",
       "39102             Minsk cultural hub becomes authorities              334   \n",
       "39103  Vatican upbeat possibility Pope Francis visiti...              210   \n",
       "39104     Indonesia buy $1.14 billion worth Russian jets              209   \n",
       "\n",
       "       title_word_count  text_sentence_count  title_sentence_count  \\\n",
       "0                    13                   28                     1   \n",
       "1                     9                   11                     1   \n",
       "2                    16                   25                     1   \n",
       "3                    15                   15                     1   \n",
       "4                    12                   19                     1   \n",
       "...                 ...                  ...                   ...   \n",
       "39100                 9                   15                     1   \n",
       "39101                 7                    6                     1   \n",
       "39102                 7                   16                     1   \n",
       "39103                 9                    8                     1   \n",
       "39104                 9                    9                     1   \n",
       "\n",
       "       text_average_word_length  title_average_word_length  \\\n",
       "0                      4.804040                   5.583333   \n",
       "1                      5.213115                   7.625000   \n",
       "2                      5.168966                   5.000000   \n",
       "3                      5.180180                   4.571429   \n",
       "4                      4.554762                   5.363636   \n",
       "...                         ...                        ...   \n",
       "39100                  5.008639                   5.888889   \n",
       "39101                  5.336066                   6.571429   \n",
       "39102                  5.044164                   6.142857   \n",
       "39103                  4.806931                   5.888889   \n",
       "39104                  5.362319                   4.888889   \n",
       "\n",
       "       text_punctuation_count  title_punctuation_count  text_stopwords_count  \\\n",
       "0                         121                        1                   186   \n",
       "1                          39                        0                   119   \n",
       "2                         148                        0                   209   \n",
       "3                         118                        2                   160   \n",
       "4                          40                        0                   195   \n",
       "...                       ...                      ...                   ...   \n",
       "39100                      68                        4                   179   \n",
       "39101                      15                        0                    45   \n",
       "39102                      45                        0                   127   \n",
       "39103                      18                        0                    81   \n",
       "39104                      27                        2                    66   \n",
       "\n",
       "       title_stopwords_count  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "...                      ...  \n",
       "39100                      1  \n",
       "39101                      1  \n",
       "39102                      2  \n",
       "39103                      2  \n",
       "39104                      2  \n",
       "\n",
       "[39105 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flesch Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating number of syllables in a word\n",
    "def nsyl(word):\n",
    "    return syllables.estimate(word) \n",
    "\n",
    "#Calculating number of syllables in a text \n",
    "def syllables_text(text):\n",
    "    syllable_count = sum(list(map(lambda w: nsyl(w), word_tokenize(text))))\n",
    "    return syllable_count\n",
    "\n",
    "data['syllables'] = data['text'].apply(syllables_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flesch-Kincaid Readability Metric\n",
    "def flesch_formula(word_count, sent_count, syllable_count):\n",
    "    if sent_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 206.835 - 1.015*word_count/sent_count - 84.6*syllable_count/word_count\n",
    "    \n",
    "# Get flesch readability\n",
    "data['flesch_readability'] = data.apply(lambda n: flesch_formula(n['text_word_count'],n['text_sentence_count'],n['syllables']),axis=1)\n",
    "data['flesch_readability'] = (data['flesch_readability'] - data['flesch_readability'].mean()) / data['flesch_readability'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    "# Calculate polarity \n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "  \n",
    "#Get subjectivity and polarity\n",
    "data['subjectivity'] = data['text'].apply(getSubjectivity)\n",
    "data['polarity'] = data['text'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_without_stopwords'].isna().sum()\n",
    "#data['title_without_stopwords'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: DeprecationWarning: invalid escape sequence '\\.'\n",
      "<>:2: DeprecationWarning: invalid escape sequence '\\.'\n",
      "<>:1: DeprecationWarning: invalid escape sequence '\\.'\n",
      "<>:2: DeprecationWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10360\\3176515627.py:1: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  data['text_without_stopwords'] = data['text_without_stopwords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10360\\3176515627.py:2: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  data['title_without_stopwords'] = data['title_without_stopwords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10360\\3176515627.py:1: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  data['text_without_stopwords'] = data['text_without_stopwords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10360\\3176515627.py:2: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  data['title_without_stopwords'] = data['title_without_stopwords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_without_stopwords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[,\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.!?]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[,\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.!?]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, x))\n\u001b[0;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mlower())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4237\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg, na_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m   4163\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4164\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4235\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   4236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4239\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4240\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py:880\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# mapper is a function\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mmap_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_values\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[,\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.!?]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[,\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.!?]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, x))\n\u001b[0;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_without_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mlower())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:209\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "data['text_without_stopwords'] = data['text_without_stopwords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "data['title_without_stopwords'] = data['title_without_stopwords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "data['text_without_stopwords'] = data['text_without_stopwords'].map(lambda x: x.lower())\n",
    "data['title_without_stopwords'] = data['title_without_stopwords'].map(lambda x: x.lower())\n",
    "data['overall_content'] = data['title_without_stopwords'] + ' ' + data['text_without_stopwords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we create a Python function to lemmatize the content in our news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "#we only consider the nouns, adjective, verbs and adverbs as these are the POS tags which give our text most contextual meaning \n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude = [\"parser\", \"ner\"])\n",
    "    output = []\n",
    "    for content in text:\n",
    "        contents = nlp(content)\n",
    "        temp = []\n",
    "        for word in contents:\n",
    "            if word.pos_ in allowed_postags: \n",
    "                temp.append(word.lemma_)\n",
    "        lemmatized_content = \" \".join(temp)\n",
    "        output.append(lemmatized_content)\n",
    "    return output \n",
    "\n",
    "texts = list(data['overall_content'])\n",
    "output = lemmatize(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a preprocessing function to tokenize our text. \n",
    "We will do this by implementing an iterative function which uses the simple_preprocess function from the gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for article in text:\n",
    "        temporary = gensim.utils.simple_preprocess(article)\n",
    "        result.append(temporary)\n",
    "    return result\n",
    "all_words = preprocess(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the results of our topic modelling later on, we also identify bigrams and trigrams in our text, which are phrases of two words and three words respectively which appear commonly in our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = gensim.models.Phrases(all_words, min_count = 5, threshold = 50) #min_count and threshold are hyperparameters we can tune later\n",
    "trigram_phases = gensim.models.Phrases(bigram_phrases[all_words], min_count = 5, threshold = 50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(bigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus_bigrams = []\n",
    "for text in all_words:\n",
    "    new_corpus_bigrams.append(bigram[text])\n",
    "    \n",
    "new_corpus_trigrams = []\n",
    "for text in all_words:\n",
    "    new_corpus_trigrams.append(trigram[bigram[text]])\n",
    "    \n",
    "for i in range(len(new_corpus_bigrams)):\n",
    "    if set(new_corpus_bigrams[i]) != set(new_corpus_trigrams[i]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently it appears that among all our texts, only one article contains trigrams. However, since all trigrams are also considered bigrams, we will use the trigrams to generate our corpus for the topic modelling classification.\n",
    "\n",
    "Following this, we generate the corpus that we will use for our topic modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(new_corpus_trigrams)\n",
    "\n",
    "corpus = []\n",
    "for text in new_corpus_trigrams:\n",
    "    frequencies = id2word.doc2bow(text) #counts occurence of each word in every document\n",
    "    corpus.append(frequencies) #stores into corpus, which becomes a list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we also filter out low value words in our corpus which do not add much meaning into the text using the TF-IDF statistical measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus, id2word = id2word)\n",
    "\n",
    "low_value = 0.05 #set a TF-IDF score of 0.05 as a threshold \n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]         \n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store corpus \n",
    "%store id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to generate our LDA model. However, we first need to determine the number of topics to train our model on. In order to determine this, we will use the coherence score metric to do. Therefore, we first implement a Python function that will allow us to determine a coherence score for a given LDA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_score(n):\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus= corpus,\n",
    "                                          id2word = id2word,\n",
    "                                          num_topics = n,\n",
    "                                          random_state = 4222, \n",
    "                                          update_every = 1,\n",
    "                                          chunksize = 2000,\n",
    "                                          passes = 10,\n",
    "                                          alpha = \"auto\")\n",
    "    coherence_model_lda = CoherenceModel(model = lda, corpus = corpus, dictionary = id2word, coherence = 'u_mass')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of possible number of topics is given below under topics_list. In addition, we will also store the coherence scores in a list for data visualisation purposes later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_list = [3,4,5,6,7,8,9,10] \n",
    "scores = [] \n",
    "\n",
    "for n in topics_list:\n",
    "    coherence_score = calculate_coherence_score(n)\n",
    "    scores.append(coherence_score)\n",
    "    print(f\"n : {n} ; Coherence Score : {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(topics_list)\n",
    "y = np.array(scores)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, the coherence score is the highest when there are 5 topics. Hence, we will train our LDA model by setting n_topics = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, \n",
    "                                            num_topics = 5,\n",
    "                                            id2word = id2word,\n",
    "                                            chunksize = 2000,\n",
    "                                            passes = 10,\n",
    "                                            update_every = 1,\n",
    "                                            alpha = 'auto',\n",
    "                                            random_state = 4222)\n",
    "\n",
    "%store lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in range(len(corpus)):\n",
    "    topic_distribution = lda_model.get_document_topics(corpus[i], minimum_probability = 0.0)\n",
    "    outputs.append(topic_distribution)\n",
    "    \n",
    "import numpy as np\n",
    "outputs = np.array(outputs).T.tolist()\n",
    "outputs = outputs[1:]\n",
    "\n",
    "data['Topic 1 Probability'] = outputs[0][0]\n",
    "data['Topic 2 Probability'] = outputs[0][1]\n",
    "data['Topic 3 Probbility'] = outputs[0][2]\n",
    "data['Topic 4 Probability'] = outputs[0][3]\n",
    "data['Topic 5 Probability'] = outputs[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorise polarity and do one-hot encoding \n",
    "data['polarity_category'] = pd.cut(x=data['polarity'], bins=[-1,-0.05,0.05,1], labels=['Negative', 'Neutral', 'Positive'])\n",
    "dummy = pd.get_dummies(data['polarity_category'], prefix='polarity_category', drop_first=True)\n",
    "data = pd.concat([data,dummy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the newly cleaned dataframe in new file\n",
    "#data.to_csv(\"processed_data.csv\", index=False) # Dataset with text feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
