{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>class</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>title_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Donald Trump wish Americans Happy New Year lea...</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>On Friday, revealed former Milwaukee Sheriff D...</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>On Christmas day, Donald Trump announced would...</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Pope Francis used annual Christmas Day message...</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Duri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  class  \\\n",
       "0  December 31, 2017      1   \n",
       "1  December 31, 2017      1   \n",
       "2  December 30, 2017      1   \n",
       "3  December 29, 2017      1   \n",
       "4  December 25, 2017      1   \n",
       "\n",
       "                              text_without_stopwords  \\\n",
       "0  Donald Trump wish Americans Happy New Year lea...   \n",
       "1  House Intelligence Committee Chairman Devin Nu...   \n",
       "2  On Friday, revealed former Milwaukee Sheriff D...   \n",
       "3  On Christmas day, Donald Trump announced would...   \n",
       "4  Pope Francis used annual Christmas Day message...   \n",
       "\n",
       "                             title_without_stopwords  \n",
       "0  Donald Trump Sends Out Embarrassing New Year’s...  \n",
       "1  Drunk Bragging Trump Staffer Started Russian C...  \n",
       "2  Sheriff David Clarke Becomes An Internet Joke ...  \n",
       "3  Trump Is So Obsessed He Even Has Obama’s Name ...  \n",
       "4  Pope Francis Just Called Out Donald Trump Duri...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy \n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "data = pd.read_csv(\"master_dataset/merged_cleaned.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30520/2372234043.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcmudict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msyllables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "#text-based features\n",
    "#word count\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "from textblob import TextBlob\n",
    "import syllables\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "def word_count(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.replace(\"\\r\\n\", ' ')\n",
    "    if text == \"no title\":\n",
    "        return 0\n",
    "    else:\n",
    "        return len(str(text).split(' '))\n",
    "\n",
    "def sentence_count(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def average_word_length(text):\n",
    "    words = text.split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        count += len(word)\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return count / len(words)\n",
    "        \n",
    "def punctuation_count(text):\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word in string.punctuation:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def stopword_count(text):\n",
    "    stopword = stopwords.words('english')\n",
    "    count = 0\n",
    "    for word in text.split():\n",
    "        if word in stopword:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "#change data type to string\n",
    "data['text'] = data['text'].astype(str)\n",
    "data['title'] = data['title'].astype(str)\n",
    "\n",
    "data['text_word_count'] = data['text'].apply(word_count)\n",
    "data['title_word_count'] = data['title'].apply(word_count)\n",
    "\n",
    "data['text_sentence_count'] = data['text'].apply(sentence_count)\n",
    "data['title_sentence_count'] = data['title'].apply(sentence_count)\n",
    "\n",
    "data['text_average_word_length'] = data['text'].apply(average_word_length)\n",
    "data['title_average_word_length'] = data['title'].apply(average_word_length)\n",
    "\n",
    "data['text_punctuation_count'] = data['text'].apply(punctuation_count)\n",
    "data['title_punctuation_count'] = data['title'].apply(punctuation_count)\n",
    "\n",
    "data['text_stopwords_count'] = data['text'].apply(stopword_count)\n",
    "data['title_stopwords_count'] = data['title'].apply(stopword_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flesch Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating number of syllables in a word\n",
    "def nsyl(word):\n",
    "    return syllables.estimate(word) \n",
    "\n",
    "#Calculating number of syllables in a text \n",
    "def syllables_text(text):\n",
    "    syllable_count = sum(list(map(lambda w: nsyl(w), word_tokenize(text))))\n",
    "    return syllable_count\n",
    "\n",
    "data['syllables'] = data['text'].apply(syllables_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flesch-Kincaid Readability Metric\n",
    "def flesch_formula(word_count, sent_count, syllable_count):\n",
    "    if sent_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 206.835 - 1.015*word_count/sent_count - 84.6*syllable_count/word_count\n",
    "    \n",
    "# Get flesch readability\n",
    "data['flesch_readability'] = data.apply(lambda n: flesch_formula(n['text_word_count'],n['text_sentence_count'],n['syllables']),axis=1)\n",
    "data['flesch_readability'] = (data['flesch_readability'] - data['flesch_readability'].mean()) / data['flesch_readability'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    "# Calculate polarity \n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "  \n",
    "#Get subjectivity and polarity\n",
    "data['subjectivity'] = data['text'].apply(getSubjectivity)\n",
    "data['polarity'] = data['text'].apply(getPolarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "data['text_without_stopwords'] = data['text_without_stopwords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "data['title_without_stopwords'] = data['title_without_stopwords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "data['text_without_stopwords'] = data['text_without_stopwords'].map(lambda x: x.lower())\n",
    "data['title_without_stopwords'] = data['title_without_stopwords'].map(lambda x: x.lower())\n",
    "data['overall_content'] = data['title_without_stopwords'] + ' ' + data['text_without_stopwords']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we create a Python function to lemmatize the content in our news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "#we only consider the nouns, adjective, verbs and adverbs as these are the POS tags which give our text most contextual meaning \n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude = [\"parser\", \"ner\"])\n",
    "    output = []\n",
    "    for content in text:\n",
    "        contents = nlp(content)\n",
    "        temp = []\n",
    "        for word in contents:\n",
    "            if word.pos_ in allowed_postags: \n",
    "                temp.append(word.lemma_)\n",
    "        lemmatized_content = \" \".join(temp)\n",
    "        output.append(lemmatized_content)\n",
    "    return output \n",
    "\n",
    "texts = list(data['overall_content'])\n",
    "output = lemmatize(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a preprocessing function to tokenize our text. \n",
    "We will do this by implementing an iterative function which uses the simple_preprocess function from the gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for article in text:\n",
    "        temporary = gensim.utils.simple_preprocess(article)\n",
    "        result.append(temporary)\n",
    "    return result\n",
    "all_words = preprocess(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the results of our topic modelling later on, we also identify bigrams and trigrams in our text, which are phrases of two words and three words respectively which appear commonly in our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = gensim.models.Phrases(all_words, min_count = 5, threshold = 50) #min_count and threshold are hyperparameters we can tune later\n",
    "trigram_phases = gensim.models.Phrases(bigram_phrases[all_words], min_count = 5, threshold = 50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(bigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus_bigrams = []\n",
    "for text in all_words:\n",
    "    new_corpus_bigrams.append(bigram[text])\n",
    "    \n",
    "new_corpus_trigrams = []\n",
    "for text in all_words:\n",
    "    new_corpus_trigrams.append(trigram[bigram[text]])\n",
    "    \n",
    "for i in range(len(new_corpus_bigrams)):\n",
    "    if set(new_corpus_bigrams[i]) != set(new_corpus_trigrams[i]):\n",
    "        print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently it appears that among all our texts, only one article contains trigrams. However, since all trigrams are also considered bigrams, we will use the trigrams to generate our corpus for the topic modelling classification.\n",
    "\n",
    "Following this, we generate the corpus that we will use for our topic modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(new_corpus_trigrams)\n",
    "\n",
    "corpus = []\n",
    "for text in new_corpus_trigrams:\n",
    "    frequencies = id2word.doc2bow(text) #counts occurence of each word in every document\n",
    "    corpus.append(frequencies) #stores into corpus, which becomes a list of lists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we also filter out low value words in our corpus which do not add much meaning into the text using the TF-IDF statistical measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus, id2word = id2word)\n",
    "\n",
    "low_value = 0.05 #set a TF-IDF score of 0.05 as a threshold \n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]         \n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store corpus \n",
    "%store id2word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to generate our LDA model. However, we first need to determine the number of topics to train our model on. In order to determine this, we will use the coherence score metric to do. Therefore, we first implement a Python function that will allow us to determine a coherence score for a given LDA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_score(n):\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus= corpus,\n",
    "                                          id2word = id2word,\n",
    "                                          num_topics = n,\n",
    "                                          random_state = 4222, \n",
    "                                          update_every = 1,\n",
    "                                          chunksize = 2000,\n",
    "                                          passes = 10,\n",
    "                                          alpha = \"auto\")\n",
    "    coherence_model_lda = CoherenceModel(model = lda, corpus = corpus, dictionary = id2word, coherence = 'u_mass')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    return coherence_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of possible number of topics is given below under topics_list. In addition, we will also store the coherence scores in a list for data visualisation purposes later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_list = [3,4,5,6,7,8,9,10] \n",
    "scores = [] \n",
    "\n",
    "for n in topics_list:\n",
    "    coherence_score = calculate_coherence_score(n)\n",
    "    scores.append(coherence_score)\n",
    "    print(f\"n : {n} ; Coherence Score : {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(topics_list)\n",
    "y = np.array(scores)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, the coherence score is the highest when there are 5 topics. Hence, we will train our LDA model by setting n_topics = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, \n",
    "                                            num_topics = 5,\n",
    "                                            id2word = id2word,\n",
    "                                            chunksize = 2000,\n",
    "                                            passes = 10,\n",
    "                                            update_every = 1,\n",
    "                                            alpha = 'auto',\n",
    "                                            random_state = 4222)\n",
    "\n",
    "%store lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in range(len(corpus)):\n",
    "    topic_distribution = lda_model.get_document_topics(corpus[i], minimum_probability = 0.0)\n",
    "    outputs.append(topic_distribution)\n",
    "    \n",
    "import numpy as np\n",
    "outputs = np.array(outputs).T.tolist()\n",
    "outputs = outputs[1:]\n",
    "\n",
    "data['Topic 1 Probability'] = outputs[0][0]\n",
    "data['Topic 2 Probability'] = outputs[0][1]\n",
    "data['Topic 3 Probbility'] = outputs[0][2]\n",
    "data['Topic 4 Probability'] = outputs[0][3]\n",
    "data['Topic 5 Probability'] = outputs[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the newly cleaned dataframe in new file\n",
    "data.to_csv(\"processed_data.csv\", index=False) # Dataset with text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorise polarity and do one-hot encoding \n",
    "data['polarity_category'] = pd.cut(x=data['polarity'], bins=[-1,-0.05,0.05,1], labels=['Negative', 'Neutral', 'Positive'])\n",
    "dummy = pd.get_dummies(data['polarity_category'], prefix='polarity_category', drop_first=True)\n",
    "data = pd.concat([data,dummy], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
