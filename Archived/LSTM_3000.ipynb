{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        donald trump sends out embarrassing new year’s...\n",
      "1        drunk bragging trump staffer started russian c...\n",
      "2        sheriff david clarke becomes an internet joke ...\n",
      "3        trump is so obsessed he even has obama’s name ...\n",
      "4        pope francis just called out donald trump duri...\n",
      "                               ...                        \n",
      "38653    'fully committed' nato backs new us approach a...\n",
      "38654    lexisnexis withdrew two products chinese marke...\n",
      "38655    minsk cultural hub becomes authorities in shad...\n",
      "38656    vatican upbeat possibility pope francis visiti...\n",
      "38657    indonesia buy $114 billion worth russian jets ...\n",
      "Name: overall_content, Length: 38658, dtype: object 0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "38653    0\n",
      "38654    0\n",
      "38655    0\n",
      "38656    0\n",
      "38657    0\n",
      "Name: class, Length: 38658, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"master_dataset/processed_data.csv\")\n",
    "# drop unwanted features\n",
    "\n",
    "\n",
    "# Drop all the column , keep only class , overall_context\n",
    "\n",
    "x = data['overall_content']\n",
    "y = data['class'] \n",
    "    \n",
    "# As we will be vectorizing the content and doing LSTM on it\n",
    "\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21196\n",
       "1    17462\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12717\n",
       "1    10477\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first split the dataset into training and test sets\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 4222)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size = 0.25, random_state = 4222)\n",
    "\n",
    "#check that train set is balance\n",
    "y_train.value_counts()\n",
    "\n",
    "# Since the dataset is pretty balanced, Real - 55% and Fake - 45% of the data,\n",
    "# By oversampling, we will have duplicates in the model which will overtrain out model.\n",
    "# By undersampling, we might lose out on critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39601\n"
     ]
    }
   ],
   "source": [
    "# Find the number of maximum text \n",
    "lst = []\n",
    "words = []\n",
    "for item in x_train:\n",
    "    lst.append(len(item.split()))\n",
    "\n",
    "big_list = [item.split() for item in x_train]\n",
    "flat_list = [item for sublist in big_list for item in sublist]\n",
    "unique = list(set(flat_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model constants.\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 3000 # Set a max length of the array, if not it will do an array of like [1,10000] , and if i would to run the LSTM, it will take 30 hours\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train.squeeze())\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train.squeeze())\n",
    "x_train = pad_sequences(tokenized_train , maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique)) # How many different words are there\n",
    "print(max(lst)) # What the maximum word in an article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   531,   251,  1883],\n",
       "       [    0,     0,     0, ...,  6672,   412,   341],\n",
       "       [    0,     0,     0, ...,  7288,  7240,  3003],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  6672,   412,   341],\n",
       "       [    0,     0,     0, ..., 12182,  7908,   300],\n",
       "       [    0,     0,     0, ...,    28,   104,  3003]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(x_validation)\n",
    "x_validation = pad_sequences(tokenized_test , maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  355,   25,   19],\n",
       "       [   0,    0,    0, ..., 3628,  412,  341],\n",
       "       [   0,    0,    0, ..., 2772,  412,  341],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 3582,  412,  341],\n",
       "       [   0,    0,    0, ..., 1239,   13,  392],\n",
       "       [   0,    0,    0, ...,   15,    1,   93]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(tokenized_test , maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  8425,    85,   548],\n",
       "       [    0,     0,     0, ...,  2814,  3091,  1620],\n",
       "       [    0,     0,     0, ...,   172,   104,  4504],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    15,     1,    93],\n",
       "       [    0,     0,     0, ...,   164,  6376,   933],\n",
       "       [    0,     0,     0, ...,    92, 10793,  4960]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "embed_size = 256\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features,embed_size))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1,activation=\"sigmoid\",name=\"predictions\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 256)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 128)         197120    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,321,281\n",
      "Trainable params: 5,321,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "725/725 [==============================] - 6511s 9s/step - loss: 0.1231 - accuracy: 0.9528 - val_loss: 0.0175 - val_accuracy: 0.9948\n",
      "Epoch 2/2\n",
      "725/725 [==============================] - 7183s 10s/step - loss: 0.0177 - accuracy: 0.9957 - val_loss: 0.9745 - val_accuracy: 0.8880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18d226b7e20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,validation_data=(x_validation,y_validation),epochs=2) #Took 4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 347s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction if the news is fake \n",
    "# Class 1 (Fake) if predicted prob >= 0.5, else class 0 (Real)\n",
    "\n",
    "y_pred = (model.predict(x_test) >= 0.5).astype(\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88      4235\n",
      "           1       0.80      1.00      0.89      3497\n",
      "\n",
      "    accuracy                           0.89      7732\n",
      "   macro avg       0.90      0.90      0.89      7732\n",
      "weighted avg       0.91      0.89      0.89      7732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis after Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725/725 [==============================] - 943s 1s/step - loss: 0.8189 - accuracy: 0.9030\n",
      "Accuracy of the model on Training Data is -  90.30352830886841 %\n",
      "242/242 [==============================] - 316s 1s/step - loss: 0.9745 - accuracy: 0.8880\n",
      "Accuracy of the model on Validation Data is -  88.79979252815247 %\n",
      "242/242 [==============================] - 312s 1s/step - loss: 0.9931 - accuracy: 0.8857\n",
      "Accuracy of the model on Testing Data is -  88.56699466705322 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100 ,  \"%\")\n",
    "print(\"Accuracy of the model on Validation Data is - \" , model.evaluate(x_validation,y_validation)[1]*100 , \"%\")\n",
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
