{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        donald trump sends out embarrassing new year’s...\n",
      "1        drunk bragging trump staffer started russian c...\n",
      "2        sheriff david clarke becomes an internet joke ...\n",
      "3        trump is so obsessed he even has obama’s name ...\n",
      "4        pope francis just called out donald trump duri...\n",
      "                               ...                        \n",
      "38653    'fully committed' nato backs new us approach a...\n",
      "38654    lexisnexis withdrew two products chinese marke...\n",
      "38655    minsk cultural hub becomes authorities in shad...\n",
      "38656    vatican upbeat possibility pope francis visiti...\n",
      "38657    indonesia buy $114 billion worth russian jets ...\n",
      "Name: overall_content, Length: 38658, dtype: object 0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "38653    0\n",
      "38654    0\n",
      "38655    0\n",
      "38656    0\n",
      "38657    0\n",
      "Name: class, Length: 38658, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"master_dataset/processed_data.csv\")\n",
    "# drop unwanted features\n",
    "\n",
    "\n",
    "# Drop all the column , keep only class , overall_context\n",
    "\n",
    "x = data['overall_content']\n",
    "y = data['class'] \n",
    "    \n",
    "# As we will be vectorizing the content and doing LSTM on it\n",
    "\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21196\n",
       "1    17462\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12717\n",
       "1    10477\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first split the dataset into training and test sets\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 4222)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size = 0.25, random_state = 4222)\n",
    "\n",
    "#check that train set is balance\n",
    "y_train.value_counts()\n",
    "\n",
    "# Since the dataset is pretty balanced, Real - 55% and Fake - 45% of the data,\n",
    "# By oversampling, we will have duplicates in the model which will overtrain out model.\n",
    "# By undersampling, we might lose out on critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39601\n"
     ]
    }
   ],
   "source": [
    "# Find the number of maximum text \n",
    "print(x_train.str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change dataframe back to Series\n",
    "\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500 # Determine how many words, you want pick to run the feature, cause if u pick a big number your LSTM will take longer to run\n",
    "embedding_dim = 128\n",
    "\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.constant(x_train.squeeze().to_list()),\n",
    "     tf.keras.utils.to_categorical(y_train.to_numpy() -1))\n",
    ").batch(1024)\n",
    "\n",
    "raw_test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.constant(x_test.squeeze().to_list()),\n",
    "     tf.keras.utils.to_categorical(y_test.to_numpy() -1))\n",
    ").batch(1024)\n",
    "\n",
    "raw_val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.constant(x_validation.squeeze().to_list()),\n",
    "     tf.keras.utils.to_categorical(y_validation.to_numpy() -1))\n",
    ").batch(1024)\n",
    "\n",
    "raw_train_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length= maxlen, # Based on how many words there are in overall_text\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = raw_train_ds.map(lambda x, y : x)\n",
    "\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['', '[UNK]', 'said', 'trump', 'the', 'us', 'would', 'president', 'i', 'people', 'it', 'one', 'state', 'new', 'also', 'donald', 'house', 'government', 'republican', 'states']\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorize_layer.get_vocabulary()))\n",
    "print(vectorize_layer.get_vocabulary()[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text,label):\n",
    "    return vectorize_layer(text),label\n",
    "\n",
    "# Vectorize the Data\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 320  289 2666 1770    1   72 3148 2246    1   72  242 2997    1  665\n",
      " 7422  251  289  589  209  143], shape=(20,), dtype=int64)\n",
      "(1024, 1)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_ds.take(1):\n",
    "    print(x[0][:20])\n",
    "    print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features,embedding_dim))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1,activation=\"sigmoid\",name=\"predictions\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 128)         131584    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,415,745\n",
      "Trainable params: 1,415,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23/23 [==============================] - 249s 11s/step - loss: 0.2240 - accuracy: 0.9610 - val_loss: 3.6213e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "23/23 [==============================] - 321s 14s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.7569e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7c4fddb50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds,validation_data=(val_ds),epochs=2)  #Took 10mins, the bigger the batch, the lesser iter needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis after Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy of the model on Training Data is - \" , model.evaluate(train_ds)[1]*100 , \"%\")\n",
    "#print(\"Accuracy of the model on Validation Data is - \" , model.evaluate(val_ds)[1]*100 , \"%\")\n",
    "#print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(test_ds)[1]*100 , \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
