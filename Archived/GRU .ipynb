{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf17c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding, Dropout\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d3bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63576ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>title_without_stopwords</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_sentence_count</th>\n",
       "      <th>title_sentence_count</th>\n",
       "      <th>text_average_word_length</th>\n",
       "      <th>...</th>\n",
       "      <th>polarity</th>\n",
       "      <th>overall_content</th>\n",
       "      <th>Topic 1 Probability</th>\n",
       "      <th>Topic 2 Probability</th>\n",
       "      <th>Topic 3 Probbility</th>\n",
       "      <th>Topic 4 Probability</th>\n",
       "      <th>Topic 5 Probability</th>\n",
       "      <th>polarity_category</th>\n",
       "      <th>polarity_category_Neutral</th>\n",
       "      <th>polarity_category_Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>1</td>\n",
       "      <td>donald trump wish americans happy new year lea...</td>\n",
       "      <td>donald trump sends out embarrassing new year’s...</td>\n",
       "      <td>516</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>4.804040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082132</td>\n",
       "      <td>donald trump sends out embarrassing new year’s...</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.747636</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.157660</td>\n",
       "      <td>0.091503</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>1</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>309</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5.213115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005004</td>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>0.064904</td>\n",
       "      <td>0.244962</td>\n",
       "      <td>0.557051</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.130763</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>1</td>\n",
       "      <td>on friday revealed former milwaukee sheriff da...</td>\n",
       "      <td>sheriff david clarke becomes an internet joke ...</td>\n",
       "      <td>600</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5.168966</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012345</td>\n",
       "      <td>sheriff david clarke becomes an internet joke ...</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.433611</td>\n",
       "      <td>0.281460</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.280524</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>on christmas day donald trump announced would ...</td>\n",
       "      <td>trump is so obsessed he even has obama’s name ...</td>\n",
       "      <td>475</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5.180180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023118</td>\n",
       "      <td>trump is so obsessed he even has obama’s name ...</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>0.788261</td>\n",
       "      <td>0.204377</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>1</td>\n",
       "      <td>pope francis used annual christmas day message...</td>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "      <td>434</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>4.554762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011722</td>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "      <td>0.292172</td>\n",
       "      <td>0.327938</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.020911</td>\n",
       "      <td>0.357842</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text  class  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...      1   \n",
       "1  House Intelligence Committee Chairman Devin Nu...      1   \n",
       "2  On Friday, it was revealed that former Milwauk...      1   \n",
       "3  On Christmas day, Donald Trump announced that ...      1   \n",
       "4  Pope Francis used his annual Christmas Day mes...      1   \n",
       "\n",
       "                              text_without_stopwords  \\\n",
       "0  donald trump wish americans happy new year lea...   \n",
       "1  house intelligence committee chairman devin nu...   \n",
       "2  on friday revealed former milwaukee sheriff da...   \n",
       "3  on christmas day donald trump announced would ...   \n",
       "4  pope francis used annual christmas day message...   \n",
       "\n",
       "                             title_without_stopwords  text_word_count  \\\n",
       "0  donald trump sends out embarrassing new year’s...              516   \n",
       "1  drunk bragging trump staffer started russian c...              309   \n",
       "2  sheriff david clarke becomes an internet joke ...              600   \n",
       "3  trump is so obsessed he even has obama’s name ...              475   \n",
       "4  pope francis just called out donald trump duri...              434   \n",
       "\n",
       "   title_word_count  text_sentence_count  title_sentence_count  \\\n",
       "0                13                   28                     1   \n",
       "1                 9                   11                     1   \n",
       "2                16                   25                     1   \n",
       "3                15                   15                     1   \n",
       "4                12                   19                     1   \n",
       "\n",
       "   text_average_word_length  ...  polarity  \\\n",
       "0                  4.804040  ...  0.082132   \n",
       "1                  5.213115  ... -0.005004   \n",
       "2                  5.168966  ... -0.012345   \n",
       "3                  5.180180  ... -0.023118   \n",
       "4                  4.554762  ... -0.011722   \n",
       "\n",
       "                                     overall_content  Topic 1 Probability  \\\n",
       "0  donald trump sends out embarrassing new year’s...             0.002194   \n",
       "1  drunk bragging trump staffer started russian c...             0.064904   \n",
       "2  sheriff david clarke becomes an internet joke ...             0.002488   \n",
       "3  trump is so obsessed he even has obama’s name ...             0.002963   \n",
       "4  pope francis just called out donald trump duri...             0.292172   \n",
       "\n",
       "   Topic 2 Probability  Topic 3 Probbility  Topic 4 Probability  \\\n",
       "0             0.747636            0.001007             0.157660   \n",
       "1             0.244962            0.557051             0.002320   \n",
       "2             0.433611            0.281460             0.001917   \n",
       "3             0.788261            0.204377             0.002290   \n",
       "4             0.327938            0.001138             0.020911   \n",
       "\n",
       "   Topic 5 Probability  polarity_category  polarity_category_Neutral  \\\n",
       "0             0.091503           Positive                          0   \n",
       "1             0.130763            Neutral                          1   \n",
       "2             0.280524            Neutral                          1   \n",
       "3             0.002109            Neutral                          1   \n",
       "4             0.357842            Neutral                          1   \n",
       "\n",
       "  polarity_category_Positive  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d95e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "keys = contraction_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531e2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    text = data['overall_content'].iloc[i]\n",
    "    for word in keys:\n",
    "        if word in data['overall_content'].iloc[i]:\n",
    "            text = text.replace(word, contraction_dict[word])\n",
    "        else:\n",
    "            pass\n",
    "    data.at[i, 'overall_content'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d841da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17157    hacking electronic voting machines easy 21st c...\n",
      "5089     federal court strikes down north carolina’s ra...\n",
      "7167     donald trump is staring down a criminal invest...\n",
      "38280    explosions rock myanmar area near bangladesh b...\n",
      "20108    trump's son close associates appear senate pre...\n",
      "                               ...                        \n",
      "22375    trump's pick lead us cftc unveils major new po...\n",
      "34804    allies press catalan leader declare full indep...\n",
      "31627    detained catalan government members say accept...\n",
      "18325    us judge orders former trump aides stay home a...\n",
      "1380     ‘lock him up’ passengers chant to disruptive m...\n",
      "Name: overall_content, Length: 30926, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['overall_content'],data['class'],test_size=0.2,random_state = 4222)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a9f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 500 # Set a max length of the array, if not it will do an array of like [1,10000] , and if i would to run the LSTM, it will take 30 hours\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train.squeeze())\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train.squeeze())\n",
    "x_train = pad_sequences(tokenized_train , maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2326be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(tokenized_test , maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9819b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         1000000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128)               88320     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,092,481\n",
      "Trainable params: 1,092,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "embed_size = 100\n",
    "\n",
    "gru_model = keras.models.Sequential()\n",
    "gru_model.add(layers.Embedding(max_features,embed_size))\n",
    "\n",
    "gru_model.add(GRU(128, return_sequences=False))\n",
    "gru_model.add(layers.Dropout(0.5))\n",
    "\n",
    "gru_model.add(layers.Dense(32, activation='relu'))\n",
    "gru_model.add(layers.Dropout(0.5))\n",
    "gru_model.add(layers.Dense(1,activation=\"sigmoid\",name=\"predictions\"))\n",
    "\n",
    "gru_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "print(gru_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e0b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "725/725 [==============================] - 365s 499ms/step - loss: 0.1450 - accuracy: 0.9505 - val_loss: 0.0476 - val_accuracy: 0.9841\n",
      "Epoch 2/2\n",
      "725/725 [==============================] - 352s 485ms/step - loss: 0.0397 - accuracy: 0.9876 - val_loss: 0.0565 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272b54855a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.fit(x_train, y_train,validation_split=0.25,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall_data = list(data['overall_content'])\n",
    "\n",
    "#import gensim\n",
    "#from gensim.utils import simple_preprocess\n",
    "\n",
    "#def preprocess(text):\n",
    "    #result = []\n",
    "    #for article in text:\n",
    "        #temporary = gensim.utils.simple_preprocess(article)\n",
    "        #result.append(temporary)\n",
    "    #return result\n",
    "\n",
    "#all_words = preprocess(lst_x_train)\n",
    "\n",
    "#lengths = [len(x) for x in all_words]\n",
    "#max_lengths = max(lengths)\n",
    "#print(max_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a59f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 35s 144ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = (gru_model.predict(x_test) >= 0.5).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a69e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a96aa82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981246766683911\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
