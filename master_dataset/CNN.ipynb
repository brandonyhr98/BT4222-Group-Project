{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.layers.convolutional import Conv1D\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processed_data.csv\")\n",
    "list(data.columns)\n",
    "# drop unwanted features\n",
    "\n",
    "data.iloc[:,5:].columns\n",
    "\n",
    "# Drop all the column , keep only class , text_without_stopwords and title_without_stopwords\n",
    "data = data.drop(data.iloc[:,5:].columns, axis=1)\n",
    "\n",
    "data = data.drop(['title', 'text'],axis=1)\n",
    "    \n",
    "# ['title', 'text', 'text_without_stopwords', 'title_without_stopwords','syllables', 'polarity_category', 'overall_content', 'polarity'], axis=1)\n",
    "#'Topic 1 Probability', 'Topic 2 Probability', 'Topic 3 Probbility' , 'Topic 4 Probability' ,'Topic 5 Probability',\n",
    "#'title_word_count', 'title_sentence_count', 'title_average_word_length','title_punctuation_count', 'title_stopwords_count'  \n",
    "# 'polarity_category_Neutral' , 'polarity_category_Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21196\n",
       "1    17462\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset is slightly imbalanced so we will perform upsampling to balance the dataset.\n",
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class', 'text_without_stopwords', 'title_without_stopwords']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>title_without_stopwords</th>\n",
       "      <th>overall_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>donald trump wish americans happy new year lea...</td>\n",
       "      <td>donald trump sends out embarrassing new year’s...</td>\n",
       "      <td>donald trump wish americans happy new year lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>on friday revealed former milwaukee sheriff da...</td>\n",
       "      <td>sheriff david clarke becomes an internet joke ...</td>\n",
       "      <td>on friday revealed former milwaukee sheriff da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>on christmas day donald trump announced would ...</td>\n",
       "      <td>trump is so obsessed he even has obama’s name ...</td>\n",
       "      <td>on christmas day donald trump announced would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>pope francis used annual christmas day message...</td>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "      <td>pope francis used annual christmas day message...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38653</th>\n",
       "      <td>0</td>\n",
       "      <td>nato allies tuesday welcomed president donald ...</td>\n",
       "      <td>'fully committed' nato backs new us approach a...</td>\n",
       "      <td>nato allies tuesday welcomed president donald ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38654</th>\n",
       "      <td>0</td>\n",
       "      <td>lexisnexis provider legal regulatory business ...</td>\n",
       "      <td>lexisnexis withdrew two products chinese market</td>\n",
       "      <td>lexisnexis provider legal regulatory business ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38655</th>\n",
       "      <td>0</td>\n",
       "      <td>in shadow disused soviet-era factories minsk s...</td>\n",
       "      <td>minsk cultural hub becomes authorities</td>\n",
       "      <td>in shadow disused soviet-era factories minsk s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38656</th>\n",
       "      <td>0</td>\n",
       "      <td>vatican secretary state cardinal pietro paroli...</td>\n",
       "      <td>vatican upbeat possibility pope francis visiti...</td>\n",
       "      <td>vatican secretary state cardinal pietro paroli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38657</th>\n",
       "      <td>0</td>\n",
       "      <td>indonesia buy 11 sukhoi fighter jets worth $11...</td>\n",
       "      <td>indonesia buy $114 billion worth russian jets</td>\n",
       "      <td>indonesia buy 11 sukhoi fighter jets worth $11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38658 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                             text_without_stopwords  \\\n",
       "0          1  donald trump wish americans happy new year lea...   \n",
       "1          1  house intelligence committee chairman devin nu...   \n",
       "2          1  on friday revealed former milwaukee sheriff da...   \n",
       "3          1  on christmas day donald trump announced would ...   \n",
       "4          1  pope francis used annual christmas day message...   \n",
       "...      ...                                                ...   \n",
       "38653      0  nato allies tuesday welcomed president donald ...   \n",
       "38654      0  lexisnexis provider legal regulatory business ...   \n",
       "38655      0  in shadow disused soviet-era factories minsk s...   \n",
       "38656      0  vatican secretary state cardinal pietro paroli...   \n",
       "38657      0  indonesia buy 11 sukhoi fighter jets worth $11...   \n",
       "\n",
       "                                 title_without_stopwords  \\\n",
       "0      donald trump sends out embarrassing new year’s...   \n",
       "1      drunk bragging trump staffer started russian c...   \n",
       "2      sheriff david clarke becomes an internet joke ...   \n",
       "3      trump is so obsessed he even has obama’s name ...   \n",
       "4      pope francis just called out donald trump duri...   \n",
       "...                                                  ...   \n",
       "38653  'fully committed' nato backs new us approach a...   \n",
       "38654    lexisnexis withdrew two products chinese market   \n",
       "38655             minsk cultural hub becomes authorities   \n",
       "38656  vatican upbeat possibility pope francis visiti...   \n",
       "38657      indonesia buy $114 billion worth russian jets   \n",
       "\n",
       "                                            overall_text  \n",
       "0      donald trump wish americans happy new year lea...  \n",
       "1      house intelligence committee chairman devin nu...  \n",
       "2      on friday revealed former milwaukee sheriff da...  \n",
       "3      on christmas day donald trump announced would ...  \n",
       "4      pope francis used annual christmas day message...  \n",
       "...                                                  ...  \n",
       "38653  nato allies tuesday welcomed president donald ...  \n",
       "38654  lexisnexis provider legal regulatory business ...  \n",
       "38655  in shadow disused soviet-era factories minsk s...  \n",
       "38656  vatican secretary state cardinal pietro paroli...  \n",
       "38657  indonesia buy 11 sukhoi fighter jets worth $11...  \n",
       "\n",
       "[38658 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"overall_text\"] = data[\"text_without_stopwords\"] + \" \" + data[\"title_without_stopwords\"]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0        12717\n",
       "1        12717\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first split the dataset into training and test sets\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['overall_text'],data['class'],test_size=0.2,random_state = 4222)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size = 0.25, random_state = 4222)\n",
    "\n",
    "\n",
    "#balance x_train with oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "# OverSampling only works on Dataframe, but current x_train and y_train are series\n",
    "x_train, y_train = oversample.fit_resample(x_train.to_frame(), y_train.to_frame())\n",
    "\n",
    "\n",
    "#check that train set is oversampled\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            overall_text\n",
      "0      venezuelans vote sunday nationwide mayoral pol...\n",
      "1      the law order candidate broke law againdonald ...\n",
      "2      the reason republican presidential frontrunner...\n",
      "3      a senior israeli minister thursday declined co...\n",
      "4      donald trump treated americans oppose enemy sa...\n",
      "...                                                  ...\n",
      "25429  randy johnson 21st century wire mirror mirror ...\n",
      "25430  well first all politicians especially elected ...\n",
      "25431  west virginia proud coal mining history rooted...\n",
      "25432  truck drivers 100% fed up illegal muslim refug...\n",
      "25433  at 8:20 friday morning first hopefully two bab...\n",
      "\n",
      "[25434 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40567\n"
     ]
    }
   ],
   "source": [
    "# Find the number of maximum text \n",
    "print(data.overall_text.str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['', '[UNK]', 'trump', 'said', 'the', 'us', 'would', 'i', 'president', 'people', 'it', 'one', 'state', 'new', 'also', 'donald', 'house', 'republican', 'government', 'he']\n"
     ]
    }
   ],
   "source": [
    "# TA example  not working for me. \n",
    "\n",
    "# Change dataframe back to Series\n",
    "# # Model constants.\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500 # Set a max length of the array, if not it will do an array of like [1,10000] , and if i would to run the LSTM, it will take 30 hours\n",
    "\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.constant(x_train.squeeze().to_list()),\n",
    "     tf.keras.utils.to_categorical(y_train.to_numpy() -1))\n",
    ").batch(2048)\n",
    "\n",
    "raw_test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.constant(x_test.squeeze().to_list()),\n",
    "     tf.keras.utils.to_categorical(y_test.to_numpy() -1))\n",
    ").batch(2048)\n",
    "\n",
    "raw_val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.constant(x_validation.squeeze().to_list()),\n",
    "     tf.keras.utils.to_categorical(y_validation.to_numpy() -1))\n",
    ").batch(2048)\n",
    "\n",
    "raw_train_ds\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=42000, # Based on how many words there are in overall_text\n",
    ")\n",
    "\n",
    "text_ds = raw_train_ds.map(lambda x, y : x)\n",
    "\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "print(len(vectorize_layer.get_vocabulary()))\n",
    "print(vectorize_layer.get_vocabulary()[:20])\n",
    "\n",
    "def vectorize_text(text,label):\n",
    "    return vectorize_layer(text),label\n",
    "\n",
    "# Vectorize the Data\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   1   75  258 3036    1  676 7608  264  340  634  221  143 1780    8\n",
      " 6478 2427 9034  178  587 9254], shape=(20,), dtype=int64)\n",
      "(2048, 1)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(x[0][:20])\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train.squeeze())\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train.squeeze())\n",
    "x_train = pad_sequences(tokenized_train , maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   74, 3354, 2415],\n",
       "       [   0,    0,    0, ...,   16, 9361,  958],\n",
       "       [   0,    0,    0, ...,  129,  536, 1240],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   61, 2170,   42],\n",
       "       [   0,    0,    0, ...,  480,  838,   42],\n",
       "       [   0,    0,    0, ...,  838, 3209, 9653]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(x_validation)\n",
    "x_validation = pad_sequences(tokenized_test , maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1177,  655,  708],\n",
       "       [   0,    0,    0, ..., 1027,  160,   19],\n",
       "       [   0,    0,    0, ...,  597,    3, 1120],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  106, 6430, 1034],\n",
       "       [   0,    0,    0, ..., 1301,  453,  928],\n",
       "       [   0,    0,    0, ...,  190, 4811,   70]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(tokenized_test , maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   40,  301,  121],\n",
       "       [   0,    0,    0, ...,  802,   17,  337],\n",
       "       [   0,    0,    0, ..., 4553, 9993,  950],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  121,  893,  211],\n",
       "       [   0,    0,    0, ..., 9927, 2736, 2663],\n",
       "       [   0,    0,    0, ...,  302,   72,  805]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "embed_size = 100\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features,embed_size))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1,activation=\"sigmoid\",name=\"predictions\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, None, 100)         1000000   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, None, 100)         0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, None, 32)          9632      \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,010,721\n",
      "Trainable params: 1,010,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25434, 500)\n",
      "(7732, 500)\n",
      "(7732, 500)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "597/597 [==============================] - 25s 40ms/step - loss: 0.2109 - accuracy: 0.9097 - val_loss: 0.0437 - val_accuracy: 0.9833\n",
      "Epoch 2/2\n",
      "597/597 [==============================] - 24s 40ms/step - loss: 0.0358 - accuracy: 0.9887 - val_loss: 0.0263 - val_accuracy: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e09933610>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.25, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "795/795 [==============================] - 32s 40ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.0250 - val_accuracy: 0.9922\n",
      "Epoch 2/2\n",
      "795/795 [==============================] - 31s 39ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0226 - val_accuracy: 0.9929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20dcbc66160>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,validation_data=(x_validation,y_validation),epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction if the news is fake \n",
    "# Class 1 (Fake) if predicted prob >= 0.5, else class 0 (Real)\n",
    "\n",
    "y_pred = (model.predict(x_test) >= 0.5).astype(\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4235\n",
      "           1       0.99      0.99      0.99      3497\n",
      "\n",
      "    accuracy                           0.99      7732\n",
      "   macro avg       0.99      0.99      0.99      7732\n",
      "weighted avg       0.99      0.99      0.99      7732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis after Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795/795 [==============================] - 4s 5ms/step - loss: 4.1196e-04 - accuracy: 1.0000\n",
      "Accuracy of the model on Training Data is -  [0.000411961053032428, 0.9999606609344482]\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.9929\n",
      "Accuracy of the model on Validation Data is -  [0.02255597896873951, 0.9928867220878601]\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.0232 - accuracy: 0.9933\n",
      "Accuracy of the model on Testing Data is -  [0.023189663887023926, 0.9932746887207031]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train))\n",
    "print(\"Accuracy of the model on Validation Data is - \" , model.evaluate(x_validation,y_validation))\n",
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
